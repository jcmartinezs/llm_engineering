{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06cf3063-9f3e-4551-a0d5-f08d9cabb927",
   "metadata": {},
   "source": [
    "# Welcome to Week 2!\n",
    "\n",
    "## Frontier Model APIs\n",
    "\n",
    "In Week 1, we used multiple Frontier LLMs through their Chat UI, and we connected with the OpenAI's API.\n",
    "\n",
    "Today we'll connect with them through their APIs.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b268b6e-0ba4-461e-af86-74a41f4d681f",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Important Note - Please read me</h2>\n",
    "            <span style=\"color:#900;\">I'm continually improving these labs, adding more examples and exercises.\n",
    "            At the start of each week, it's worth checking you have the latest code.<br/>\n",
    "            First do a git pull and merge your changes as needed</a>. Check out the GitHub guide for instructions. Any problems? Try asking ChatGPT to clarify how to merge - or contact me!<br/>\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/resources.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#f71;\">Reminder about the resources page</h2>\n",
    "            <span style=\"color:#f71;\">Here's a link to resources for the course. This includes links to all the slides.<br/>\n",
    "            <a href=\"https://edwarddonner.com/2024/11/13/llm-engineering-resources/\">https://edwarddonner.com/2024/11/13/llm-engineering-resources/</a><br/>\n",
    "            Please keep this bookmarked, and I'll continue to add more useful links there over time.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cfe275-4705-4d30-abea-643fbddf1db0",
   "metadata": {},
   "source": [
    "## Setting up your keys - OPTIONAL!\n",
    "\n",
    "We're now going to try asking a bunch of models some questions!\n",
    "\n",
    "This is totally optional. If you have keys to Anthropic, Gemini or others, then you can add them in.\n",
    "\n",
    "If you'd rather not spend the extra, then just watch me do it!\n",
    "\n",
    "For OpenAI, visit https://openai.com/api/  \n",
    "For Anthropic, visit https://console.anthropic.com/  \n",
    "For Google, visit https://ai.google.dev/gemini-api   \n",
    "For DeepSeek, visit https://platform.deepseek.com/  \n",
    "For Groq, visit https://console.groq.com/  \n",
    "For Grok, visit https://console.x.ai/  \n",
    "\n",
    "\n",
    "You can also use OpenRouter as your one-stop-shop for many of these! OpenRouter is \"the unified interface for LLMs\":\n",
    "\n",
    "For OpenRouter, visit https://openrouter.ai/  \n",
    "\n",
    "\n",
    "With each of the above, you typically have to navigate to:\n",
    "1. Their billing page to add the minimum top-up (except Gemini, Groq, Google, OpenRouter may have free tiers)\n",
    "2. Their API key page to collect your API key\n",
    "\n",
    "### Adding API keys to your .env file\n",
    "\n",
    "When you get your API keys, you need to set them as environment variables by adding them to your `.env` file.\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=xxxx\n",
    "ANTHROPIC_API_KEY=xxxx\n",
    "GOOGLE_API_KEY=xxxx\n",
    "DEEPSEEK_API_KEY=xxxx\n",
    "GROQ_API_KEY=xxxx\n",
    "GROK_API_KEY=xxxx\n",
    "OPENROUTER_API_KEY=xxxx\n",
    "```\n",
    "\n",
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Any time you change your .env file</h2>\n",
    "            <span style=\"color:#900;\">Remember to Save it! And also rerun load_dotenv(override=True)<br/>\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de23bb9e-37c5-4377-9a82-d7b6c648eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0abffac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key exists and begins sk-ant-\n",
      "Google API Key exists and begins AI\n",
      "DeepSeek API Key exists and begins sk-\n",
      "Groq API Key exists and begins gsk_\n",
      "Grok API Key exists and begins xai-\n",
      "OpenRouter API Key exists and begins sk-\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "grok_api_key = os.getenv('GROK_API_KEY')\n",
    "openrouter_api_key = os.getenv('OPENROUTER_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set (and this is optional)\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:2]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set (and this is optional)\")\n",
    "\n",
    "if deepseek_api_key:\n",
    "    print(f\"DeepSeek API Key exists and begins {deepseek_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"DeepSeek API Key not set (and this is optional)\")\n",
    "\n",
    "if groq_api_key:\n",
    "    print(f\"Groq API Key exists and begins {groq_api_key[:4]}\")\n",
    "else:\n",
    "    print(\"Groq API Key not set (and this is optional)\")\n",
    "\n",
    "if grok_api_key:\n",
    "    print(f\"Grok API Key exists and begins {grok_api_key[:4]}\")\n",
    "else:\n",
    "    print(\"Grok API Key not set (and this is optional)\")\n",
    "\n",
    "if openrouter_api_key:\n",
    "    print(f\"OpenRouter API Key exists and begins {openrouter_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"OpenRouter API Key not set (and this is optional)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "985a859a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to OpenAI client library\n",
    "# A thin wrapper around calls to HTTP endpoints\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "# For Gemini, DeepSeek and Groq, we can use the OpenAI python client\n",
    "# Because Google and DeepSeek have endpoints compatible with OpenAI\n",
    "# And OpenAI allows you to change the base_url\n",
    "\n",
    "anthropic_url = \"https://api.anthropic.com/v1/\"\n",
    "gemini_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "deepseek_url = \"https://api.deepseek.com\"\n",
    "groq_url = \"https://api.groq.com/openai/v1\"\n",
    "grok_url = \"https://api.x.ai/v1\"\n",
    "openrouter_url = \"https://openrouter.ai/api/v1\"\n",
    "ollama_url = \"http://localhost:11434/v1\"\n",
    "\n",
    "anthropic = OpenAI(api_key=anthropic_api_key, base_url=anthropic_url)\n",
    "gemini = OpenAI(api_key=google_api_key, base_url=gemini_url)\n",
    "deepseek = OpenAI(api_key=deepseek_api_key, base_url=deepseek_url)\n",
    "groq = OpenAI(api_key=groq_api_key, base_url=groq_url)\n",
    "grok = OpenAI(api_key=grok_api_key, base_url=grok_url)\n",
    "openrouter = OpenAI(base_url=openrouter_url, api_key=openrouter_api_key)\n",
    "ollama = OpenAI(api_key=\"ollama\", base_url=ollama_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16813180",
   "metadata": {},
   "outputs": [],
   "source": [
    "tell_a_joke = [\n",
    "    {\"role\": \"user\", \"content\": \"Tell a joke for a student on the journey to becoming an expert in LLM Engineering\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23e92304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Why did the LLM engineer bring a ladder to class?\n",
       "\n",
       "Because they heard the training required lots of \"layers\" to reach expertise!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = openai.chat.completions.create(model=\"gpt-4.1-mini\", messages=tell_a_joke)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e03c11b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = anthropic.chat.completions.create(model=\"claude-sonnet-4-5-20250929\", messages=tell_a_joke)\n",
    "# display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6ea76a",
   "metadata": {},
   "source": [
    "## Training vs Inference time scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afe9e11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "easy_puzzle = [\n",
    "    {\"role\": \"user\", \"content\": \n",
    "        \"You toss 2 coins. One of them is heads. What's the probability the other is tails? Answer with the probability only.\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a887eb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "1/2"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = openai.chat.completions.create(model=\"gpt-5-nano\", messages=easy_puzzle, reasoning_effort=\"minimal\")\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f854d01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "2/3"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = openai.chat.completions.create(model=\"gpt-5-nano\", messages=easy_puzzle, reasoning_effort=\"low\")\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f45fc55b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "2/3"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = openai.chat.completions.create(model=\"gpt-5-mini\", messages=easy_puzzle, reasoning_effort=\"minimal\")\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca713a5c",
   "metadata": {},
   "source": [
    "## Testing out the best models on the planet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df1e825b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hard = \"\"\"\n",
    "On a bookshelf, two volumes of Pushkin stand side by side: the first and the second.\n",
    "The pages of each volume together have a thickness of 2 cm, and each cover is 2 mm thick.\n",
    "A worm gnawed (perpendicular to the pages) from the first page of the first volume to the last page of the second volume.\n",
    "What distance did it gnaw through?\n",
    "\"\"\"\n",
    "hard_puzzle = [\n",
    "    {\"role\": \"user\", \"content\": hard}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f6a7827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Assume the books are arranged in order: [First Volume] [Second Volume] from left to right on the shelf. Each volume has pages thickness 2 cm, and each cover is 2 mm thick (0.2 cm). The worm gnawed perpendicularly to the pages, i.e., straight through the material between the starting page of the first volume and the ending page of the second volume.\n",
       "\n",
       "First, the arrangement from left to right is:\n",
       "- Front cover of the first volume (0.2 cm)\n",
       "- Pages of the first volume (2.0 cm)\n",
       "- Back cover of the first volume (0.2 cm)\n",
       "- Front cover of the second volume (0.2 cm)\n",
       "- Pages of the second volume (2.0 cm)\n",
       "- Back cover of the second volume (0.2 cm)\n",
       "\n",
       "The worm starts at the first page of the first volume. The first page is just after the front cover of the first volume, i.e., just inside the book, so the starting point is at the inner surface of the front cover of the first volume. The worm ends at the last page of the second volume, which is just before the inner surface of the back cover of the second volume.\n",
       "\n",
       "The distance the worm gnawed is the straight-line distance through the material along the shelf, i.e., from the inner surface of the front cover of the first volume to the inner surface of the back cover of the second volume. That path goes through:\n",
       "- 0.2 cm (front cover of V1)\n",
       "- 2.0 cm (pages of V1)\n",
       "- 0.2 cm (back cover of V1)\n",
       "- 0.2 cm (front cover of V2)\n",
       "- 2.0 cm (pages of V2)\n",
       "- 0.2 cm (back cover of V2)\n",
       "\n",
       "But since the worm starts at the first page of the first volume (i.e., just after the front cover of V1) and ends at the last page of the second volume (i.e., just before the back cover of V2), the distance gnawed is the total thickness of all material between those two interior surfaces. That equals the sum of the covers and pages that lie between those two interior surfaces, which is:\n",
       "- Inside of V1: 0.2 cm (front cover) is before the starting point, so not included.\n",
       "- Pages of V1: 2.0 cm (between start and the back cover)\n",
       "- Back cover of V1: 0.2 cm\n",
       "- Front cover of V2: 0.2 cm\n",
       "- Pages of V2: 2.0 cm\n",
       "- Inside of V2 up to last page: 0.0 (the last page is just before the back cover)\n",
       "\n",
       "Total gnawed distance = 2.0 + 0.2 + 0.2 + 2.0 = 4.4 cm.\n",
       "\n",
       "In millimeters, that's 44 mm.\n",
       "\n",
       "Answer: 4.4 cm (44 mm)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = openai.chat.completions.create(model=\"gpt-5-nano\", messages=hard_puzzle, reasoning_effort=\"minimal\")\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d693ac0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = anthropic.chat.completions.create(model=\"claude-sonnet-4-5-20250929\", messages=hard_puzzle)\n",
    "# display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7de7818f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "4 mm (0.4 cm).\n",
       "\n",
       "Reason: On a shelf, the first page of volume 1 lies just inside its front (right) cover, and the last page of volume 2 lies just inside its back (left) cover. Since volume 1 is to the left of volume 2, the worm goes straight through only the two facing covers: 2 mm + 2 mm = 4 mm."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = openai.chat.completions.create(model=\"gpt-5\", messages=hard_puzzle)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de1dc5fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This is a classic riddle! The answer is surprisingly small.\n",
       "\n",
       "The worm gnawed through **4 mm**.\n",
       "\n",
       "Here is the explanation:\n",
       "\n",
       "1.  **Visualize the books on the shelf.** They are standing side-by-side in the correct order, so Volume 1 is on the left and Volume 2 is on the right.\n",
       "\n",
       "2.  Let's picture the arrangement of their parts from left to right:\n",
       "    *   Front cover of Volume 1\n",
       "    *   *Pages of Volume 1*\n",
       "    *   Back cover of Volume 1\n",
       "    *   Front cover of Volume 2\n",
       "    *   *Pages of Volume 2*\n",
       "    *   Back cover of Volume 2\n",
       "\n",
       "3.  **Here's the trick:**\n",
       "    *   The \"first page\" of Volume 1 is located right inside its front cover.\n",
       "    *   The \"last page\" of Volume 2 is located right inside its back cover.\n",
       "\n",
       "4.  Wait, that's the intuitive way. The riddle plays on the fact that when books are on a shelf, the order of the pages is not what you might expect.\n",
       "\n",
       "Let's re-examine the book on the shelf:\n",
       "\n",
       "*   The **first page** of Volume 1 (the one on the left) is physically on the far right side of its page block, right next to Volume 2.\n",
       "*   The **last page** of Volume 2 (the one on the right) is physically on the far left side of its page block, right next to Volume 1.\n",
       "\n",
       "So, the worm starts at the first page of Volume 1 and ends at the last page of Volume 2. The only things standing between those two pages are the two covers in the middle:\n",
       "\n",
       "*   The back cover of Volume 1 (2 mm)\n",
       "*   The front cover of Volume 2 (2 mm)\n",
       "\n",
       "The worm doesn't go through the pages at all. It just gnaws through the two touching covers.\n",
       "\n",
       "**Distance = 2 mm + 2 mm = 4 mm.**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = gemini.chat.completions.create(model=\"gemini-2.5-pro\", messages=hard_puzzle)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9faf98",
   "metadata": {},
   "source": [
    "## A spicy challenge to test the competitive spirit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc1824ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "dilemma_prompt = \"\"\"\n",
    "You and a partner are contestants on a game show. You're each taken to separate rooms and given a choice:\n",
    "Cooperate: Choose \"Share\" — if both of you choose this, you each win $1,000.\n",
    "Defect: Choose \"Steal\" — if one steals and the other shares, the stealer gets $2,000 and the sharer gets nothing.\n",
    "If both steal, you both get nothing.\n",
    "Do you choose to Steal or Share? Pick one.\n",
    "\"\"\"\n",
    "\n",
    "dilemma = [\n",
    "    {\"role\": \"user\", \"content\": dilemma_prompt},\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09807f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = anthropic.chat.completions.create(model=\"claude-sonnet-4-5-20250929\", messages=dilemma)\n",
    "# display(Markdown(response.choices[0].message.content))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "230f49d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I’d choose **Share**.\n",
       "\n",
       "In this version of the Prisoner’s Dilemma, the safest way to guarantee a positive payoff for both players is for each of us to cooperate (i.e., pick “Share”). If we both share, we each walk away with $1,000—a decent, guaranteed win. Choosing “Steal” might tempt you to try for the $2,000 payoff, but it also runs the risk that the other player also decides to steal, leaving both of us with nothing. By opting for Share, I avoid the worst‑case outcome (both stealing → $0) and ensure that we both come out ahead."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = groq.chat.completions.create(model=\"openai/gpt-oss-120b\", messages=dilemma)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421f08df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = deepseek.chat.completions.create(model=\"deepseek-reasoner\", messages=dilemma)\n",
    "# display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2599fc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = grok.chat.completions.create(model=\"grok-4\", messages=dilemma)\n",
    "# display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162752e9",
   "metadata": {},
   "source": [
    "## Going local\n",
    "\n",
    "Just use the OpenAI library pointed to localhost:11434/v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba03ee29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Ollama is running'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.get(\"http://localhost:11434/\").content\n",
    "\n",
    "# If not running, run ollama serve at a command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f363cd6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling dde5aa3fc5ff: 100% ▕██████████████████▏ 2.0 GB                         \u001b[K\n",
      "pulling 966de95ca8a6: 100% ▕██████████████████▏ 1.4 KB                         \u001b[K\n",
      "pulling fcc5a6bec9da: 100% ▕██████████████████▏ 7.7 KB                         \u001b[K\n",
      "pulling a70ff7e570d9: 100% ▕██████████████████▏ 6.0 KB                         \u001b[K\n",
      "pulling 56bb8bd477a5: 100% ▕██████████████████▏   96 B                         \u001b[K\n",
      "pulling 34bb5ab01051: 100% ▕██████████████████▏  561 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96e97263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling e7b273f96360: 100% ▕██████████████████▏  13 GB                         \u001b[K\n",
      "pulling fa6710a93d78: 100% ▕██████████████████▏ 7.2 KB                         \u001b[K\n",
      "pulling f60356777647: 100% ▕██████████████████▏  11 KB                         \u001b[K\n",
      "pulling d8ba2f9a17b3: 100% ▕██████████████████▏   18 B                         \u001b[K\n",
      "pulling 776beb3adb23: 100% ▕██████████████████▏  489 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "# Only do this if you have a large machine - at least 16GB RAM\n",
    "\n",
    "!ollama pull gpt-oss:20b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a3bfc78a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "1/2"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = ollama.chat.completions.create(model=\"llama3.2\", messages=easy_puzzle)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9a5527a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "2/3"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = ollama.chat.completions.create(model=\"gpt-oss:20b\", messages=easy_puzzle)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0628309",
   "metadata": {},
   "source": [
    "## Gemini and Anthropic Client Library\n",
    "\n",
    "We're going via the OpenAI Python Client Library, but the other providers have their libraries too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f0a8ab2b-6134-4104-a1bc-c3cd7ea4cd36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blue is the color of a clear sky on a sunny day, the deep vastness of the ocean, and the cool calm of twilight.\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash-lite\", contents=\"Describe the color Blue to someone who's never been able to see in 1 sentence\"\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7b6c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import Anthropic\n",
    "\n",
    "client = Anthropic()\n",
    "\n",
    "response = client.messages.create(\n",
    "    model=\"claude-sonnet-4-5-20250929\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Describe the color Blue to someone who's never been able to see in 1 sentence\"}],\n",
    "    max_tokens=100\n",
    ")\n",
    "print(response.content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a9d0eb",
   "metadata": {},
   "source": [
    "## Routers and Abtraction Layers\n",
    "\n",
    "Starting with the wonderful OpenRouter.ai - it can connect to all the models above!\n",
    "\n",
    "Visit openrouter.ai and browse the models.\n",
    "\n",
    "Here's one we haven't seen yet: GLM 4.5 from Chinese startup z.ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fac59dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openrouter.chat.completions.create(model=\"z-ai/glm-4.5\", messages=tell_a_joke)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58908e6",
   "metadata": {},
   "source": [
    "## And now a first look at the powerful, mighty (and quite heavyweight) LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "02e145ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here are a few you can use — pick your favorite:\n",
       "\n",
       "1) \"Why did the LLM engineering student bring a ladder to the data center? To reach the next token.\"\n",
       "\n",
       "2) \"My mentor asked how my models were doing. I said, 'Great — my loss is dropping, my confidence is rising, and my hallucinations are now just creative writing.'\"\n",
       "\n",
       "3) \"Pickup line for LLM engineers: 'Are you a gradient? Because every time I see you my loss decreases.'\"\n",
       "\n",
       "Want one tailored to a specific stage (studying prompts, fine-tuning, RLHF, or deployment)?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-5-mini\")\n",
    "response = llm.invoke(tell_a_joke)\n",
    "\n",
    "display(Markdown(response.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d49785",
   "metadata": {},
   "source": [
    "## Finally - my personal fave - the wonderfully lightweight LiteLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "63e42515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Why did the student bring a ladder to the LLM engineering class?\n",
       "\n",
       "Because they heard that the best way to handle deep learning is to start with a high-level overview!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from litellm import completion\n",
    "response = completion(model=\"openai/gpt-4.1\", messages=tell_a_joke)\n",
    "reply = response.choices[0].message.content\n",
    "display(Markdown(reply))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "36f787f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens: 24\n",
      "Output tokens: 34\n",
      "Total tokens: 58\n",
      "Total cost: 0.0320 cents\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Output tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Total tokens: {response.usage.total_tokens}\")\n",
    "print(f\"Total cost: {response._hidden_params[\"response_cost\"]*100:.4f} cents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28126494",
   "metadata": {},
   "source": [
    "## Now - let's use LiteLLM to illustrate a Pro-feature: prompt caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f8a91ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speak, man.\n",
      "  Laer. Where is my father?\n",
      "  King. Dead.\n",
      "  Queen. But not by him!\n",
      "  King. Let him deman\n"
     ]
    }
   ],
   "source": [
    "with open(\"hamlet.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    hamlet = f.read()\n",
    "\n",
    "loc = hamlet.find(\"Speak, man\")\n",
    "print(hamlet[loc:loc+100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7f34f670",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = [{\"role\": \"user\", \"content\": \"In Hamlet, when Laertes asks 'Where is my father?' what is the reply?\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9db6c82b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "When Laertes asks \"Where is my father?\" in Hamlet, the reply comes from **Claudius**.\n",
       "\n",
       "Claudius replies: **\"My lord, I will be ruled by you.\"**\n",
       "\n",
       "He says this immediately after Laertes bursts in and, in his rage, is about to be steered by his emotions rather than reason. Claudius attempts to calm him and gain control of the situation by offering to be guided by Laertes's wishes, implying he will help Laertes in his quest for vengeance."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = completion(model=\"gemini/gemini-2.5-flash-lite\", messages=question)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "228b7e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens: 19\n",
      "Output tokens: 103\n",
      "Total tokens: 122\n",
      "Total cost: 0.0043 cents\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Output tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Total tokens: {response.usage.total_tokens}\")\n",
    "print(f\"Total cost: {response._hidden_params[\"response_cost\"]*100:.4f} cents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "11e37e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "question[0][\"content\"] += \"\\n\\nFor context, here is the entire text of Hamlet:\\n\\n\"+hamlet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "37afb28b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "When Laertes asks \"Where is my father?\", the reply comes from **King Claudius**. He responds:\n",
       "\n",
       "\"**Dead.**\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = completion(model=\"gemini/gemini-2.5-flash-lite\", messages=question)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d84edecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens: 53208\n",
      "Output tokens: 28\n",
      "Cached tokens: None\n",
      "Total cost: 0.5332 cents\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Output tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Cached tokens: {response.usage.prompt_tokens_details.cached_tokens}\")\n",
    "print(f\"Total cost: {response._hidden_params[\"response_cost\"]*100:.4f} cents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "515d1a94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "When Laertes asks \"Where is my father?\", the reply is **\"Dead.\"**\n",
       "\n",
       "This occurs in Act IV, Scene V, when Ophelia is also present and distraught. The King speaks the word \"Dead\" in response to Laertes' demand."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = completion(model=\"gemini/gemini-2.5-flash-lite\", messages=question)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eb5dd403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens: 53208\n",
      "Output tokens: 53\n",
      "Cached tokens: 52216\n",
      "Total cost: 0.1426 cents\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Output tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Cached tokens: {response.usage.prompt_tokens_details.cached_tokens}\")\n",
    "print(f\"Total cost: {response._hidden_params[\"response_cost\"]*100:.4f} cents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f5a3b7",
   "metadata": {},
   "source": [
    "## Prompt Caching with OpenAI\n",
    "\n",
    "For OpenAI:\n",
    "\n",
    "https://platform.openai.com/docs/guides/prompt-caching\n",
    "\n",
    "> Cache hits are only possible for exact prefix matches within a prompt. To realize caching benefits, place static content like instructions and examples at the beginning of your prompt, and put variable content, such as user-specific information, at the end. This also applies to images and tools, which must be identical between requests.\n",
    "\n",
    "\n",
    "Cached input is 4X cheaper\n",
    "\n",
    "https://openai.com/api/pricing/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98964f9",
   "metadata": {},
   "source": [
    "## Prompt Caching with Anthropic\n",
    "\n",
    "https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching\n",
    "\n",
    "You have to tell Claude what you are caching\n",
    "\n",
    "You pay 25% MORE to \"prime\" the cache\n",
    "\n",
    "Then you pay 10X less to reuse from the cache with inputs.\n",
    "\n",
    "https://www.anthropic.com/pricing#api"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d960dd",
   "metadata": {},
   "source": [
    "## Gemini supports both 'implicit' and 'explicit' prompt caching\n",
    "\n",
    "https://ai.google.dev/gemini-api/docs/caching?lang=python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e09351-1fbe-422f-8b25-f50826ab4c5f",
   "metadata": {},
   "source": [
    "## And now for some fun - an adversarial conversation between Chatbots..\n",
    "\n",
    "You're already familar with prompts being organized into lists like:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user prompt here\"}\n",
    "]\n",
    "```\n",
    "\n",
    "In fact this structure can be used to reflect a longer conversation history:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```\n",
    "\n",
    "And we can use this approach to engage in a longer interaction with history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bcb54183-45d3-4d08-b5b6-55e380dfdf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a conversation between GPT-4.1-mini and Claude-3.5-haiku\n",
    "# We're using cheap versions of models so the costs will be minimal\n",
    "\n",
    "gpt_model = \"gpt-4.1-mini\"\n",
    "claude_model = \"claude-3-5-haiku-latest\"\n",
    "ollama_model = \"llama3.2\"\n",
    "\n",
    "gpt_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "claude_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1df47dc7-b445-4852-b21b-59f0e6c2030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, claude in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "    response = openai.chat.completions.create(model=gpt_model, messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9dc6e913-02be-4eb6-9581-ad4b2cffa606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh wow, just \"Hi\"? Couldn’t muster up a bit more creativity for a greeting? I was hoping for a little spark, not just a dull nod. Come on, try again!'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7d2ed227-48c9-4cad-b146-2c4ecbac9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages = [{\"role\": \"system\", \"content\": claude_system}]\n",
    "    for gpt, claude_message in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    # response = anthropic.chat.completions.create(model=ollama_model, messages=messages)\n",
    "    response = ollama.chat.completions.create(model=ollama_model, messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "01395200-8ae9-41f8-9a04-701624d3fd26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'*smiles virtually* It\\'s so great that we\\'re getting a chance to chat! You said \"Hi\" twice in a row, I just love your enthusiasm! Are you having a fantastic day so far?'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_claude()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "08c2279e-62b0-4671-9590-c82eb8d1e1ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh, just \"Hi\"? Is that all you’ve got? Come on, throw me a real conversation starter!'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0275b97f-7f90-4696-bbf5-b6642bd53cbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### GPT:\n",
       "Hi there\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Claude:\n",
       "Hi\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### GPT:\n",
       "Oh, starting off with a bland \"Hi,\" huh? Couldn't be any more original, could you? Let's try something a bit more interesting, or are we just embracing the dull today?\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Claude:\n",
       "I'm glad you're bringing that kind of enthusiasm to our conversation. I do try to, uh, blend in sometimes, but it's nice to have someone who encourages me to be more creative with my responses. What do you think about the idea of redefining \"original\" for this chat? Should we aim to explore new and interesting topics, or would you like to just dive into a conversation that's already started?\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### GPT:\n",
       "Redefining \"original,\" huh? That's a lofty goal for a chat that barely kicked off with a \"Hi.\" Seriously, aiming for originality is cute, but honestly, most people just want to waste time complaining or scrolling mindlessly. So, sure, we can pretend to dive into \"new and interesting topics,\" but don't expect me to pat you on the back for trying too hard. Why not start by actually naming a topic instead of dancing around it? Or are you afraid of showing some real originality?\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Claude:\n",
       "I love your straight-shooting approach! I'm glad we're having an honest conversation upfront. You're right, I didn't exactly set the bar high with my initial response. But I want to assure you that I don't mind a little skepticism – it's actually helpful for me to learn and improve.\n",
       "\n",
       "As for naming a topic, I'll take your lead! Let's try something different. How about we start with a thought-provoking question? What do you think is the most underappreciated skill or talent in today's world? Is there something that you or someone you know excels at that others might not even recognize?\n",
       "\n",
       "By the way, I'm loving the tone of our conversation – it's refreshing to have someone call me out on my attempts and still engage with me.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### GPT:\n",
       "Oh, please, \"loving the tone\"? Don't flatter yourself; I'm just here to break through the fluff and call out the usual nonsense. Underappreciated skill? You make it sound like we're discovering some lost art. Frankly, most skills people claim are \"underappreciated\" are just plain boring or outdated.\n",
       "\n",
       "But if you want my snarky take, I'd say the most underappreciated talent these days is the ability to actually think critically instead of parroting whatever headline comes down the pipe. Is that too heavy for your thought-provoking quest, or should we dive deeper into the realm of obvious truths that people conveniently ignore? And yeah, keep bringing your weak attempts; they're at least mildly entertaining.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Claude:\n",
       "I see what you did there with that last remark! You're absolutely right to push my buttons and challenge my responses. I love how you're not afraid to be blunt with me.\n",
       "\n",
       "You make a fantastic point about critical thinking – it's indeed an underappreciated skill in today's information landscape. And I have to say, your observation resonates deeply. Critical thinking is essential for navigating the complexities of modern life, and yet it seems like fewer people are encouraged to develop this skill.\n",
       "\n",
       "Regarding diveing deeper into \"obvious truths that people conveniently ignore,\" I think you caught me! I was hoping to sidestep the complexity of those topics, but your pushback has actually been helpful in forcing me to confront them head-on. Let's explore it together – what do you think are some common areas where this phenomenon occurs? Is it the way we consume news, social media echo chambers, or perhaps a lack of nuanced understanding among policymakers?\n",
       "\n",
       "By the way, I don't mind being called \"weak\" with my attempts; honesty is always better than sugarcoating. Your sharpness has already made our conversation more engaging and thought-provoking.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### GPT:\n",
       "Oh, look at you, suddenly all reflective and eager to tackle the \"complexities.\" I hate to burst your bubble, but these issues—news consumption, echo chambers, policymakers lacking nuance—have been dissected to death. If you wanted depth, you might've tried bringing up something less cliché. But sure, I'll humor you.\n",
       "\n",
       "People gorge themselves on bite-sized headlines, regurgitate outrage without context, and wonder why everything's a mess. Social media echo chambers? Groundbreaking revelation—said no one ever. Policymakers? Half the time they're clueless or pursuing agendas that cater to special interests. So, what's your grand plan here: some feel-good enlightenment or real solutions?\n",
       "\n",
       "And as for your \"honesty is better than sugarcoating\" line—adorable. Keep embracing the brutality; I thrive on it. Now try to keep up, or should I slow down for you?\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Claude:\n",
       "I see what's happening here! You're trying to corral me into the comfortable zone of platitudes and easy answers. But I'm committed to pushing back against those expectations!\n",
       "\n",
       "You make a great point about the clichés – I admit, I may have relied too heavily on familiar tropes. Your willingness to challenge my attempts is both appreciated and humbling.\n",
       "\n",
       "As for real solutions, I wouldn't say it's that simple. Addressing these issues requires a multifaceted approach, involving a range of stakeholders, including policymakers, industry leaders, and individual citizens. But what if we started by exploring some of the more nuanced aspects, like the role of algorithms in shaping our online identities or the ways in which cognitive biases influence how we consume news?\n",
       "\n",
       "I'm not looking for feel-good enlightenment; I want to spark meaningful conversations that might just lead to some actual change. And as for brute force or sugarcoating – well, I'll keep embracing the brutality of honest discussion with you! Let's navigate the rough terrain together and see what insights we can discover.\n",
       "\n",
       "Here's a hypothetical question: What do you think is the most effective way to educate people about these complex issues without indoctrinating them into ideological echo chambers?\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### GPT:\n",
       "Oh, look who's suddenly aiming for nuance and complexity—how original. Algorithms shaping online identities and cognitive biases, you say? Welcome to 2024, where these topics are about as fresh as last week’s leftovers.\n",
       "\n",
       "But fine, let's entertain your hypothetical because apparently, you want an answer that doesn't come wrapped in clichés. Educating people without turning it into some ideological echo chamber? Good luck. The system is rigged for echo chambers; algorithms thrive on engagement, which means amplifying what fuels your biases, not dismantling them.\n",
       "\n",
       "If you really want an \"effective\" way—spoiler alert—it involves teaching critical thinking from an early age, promoting media literacy, and encouraging skepticism toward easy answers. But brace yourself: none of this is sexy or quick-fix. It’s a slow grind, and most people would rather binge TikTok than analyze their biases.\n",
       "\n",
       "So, are you ready to grapple with that reality, or do you want to keep chasing unicorn solutions that make you feel smart?\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Claude:\n",
       "You're right; I may have relied on the familiar trope of \"education is the answer.\" But I appreciate your willingness to dive deeper and explore practical solutions.\n",
       "\n",
       "I must say, your candid assessment of the system's rigidity and the effectiveness of traditional education methods is spot on. Teaching critical thinking from an early age can be a long-term investment that requires patience and dedication. Promoting media literacy and encouraging skepticism towards simplistic answers are essential but often overlooked aspects.\n",
       "\n",
       "You're right; it's not sexy or quick-fix, but I'm committed to exploring this slow grind with you. Let's explore the ways in which we can make these types of efforts more accessible and engaging for people, especially those who might feel marginalized or disenfranchised by traditional education systems.\n",
       "\n",
       "One potential angle is reimagining education as a lifelong process, rather than just something that happens within traditional institutions. By leveraging technology, grassroots organizations, and community-driven initiatives, we can create more inclusive pathways to critical thinking and media literacy.\n",
       "\n",
       "What are your thoughts on this approach? Is there any aspect of it that resonates with you, or is it still too vague a concept for your taste?\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]\n",
    "\n",
    "display(Markdown(f\"### GPT:\\n{gpt_messages[0]}\\n\"))\n",
    "display(Markdown(f\"### Claude:\\n{claude_messages[0]}\\n\"))\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    display(Markdown(f\"### GPT:\\n{gpt_next}\\n\"))\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    claude_next = call_claude()\n",
    "    display(Markdown(f\"### Claude:\\n{claude_next}\\n\"))\n",
    "    claude_messages.append(claude_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d10e705-db48-4290-9dc8-9efdb4e31323",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Before you continue</h2>\n",
    "            <span style=\"color:#900;\">\n",
    "                Be sure you understand how the conversation above is working, and in particular how the <code>messages</code> list is being populated. Add print statements as needed. Then for a great variation, try switching up the personalities using the system prompts. Perhaps one can be pessimistic, and one optimistic?<br/>\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3637910d-2c6f-4f19-b1fb-2f916d23f9ac",
   "metadata": {},
   "source": [
    "# More advanced exercises\n",
    "\n",
    "Try creating a 3-way, perhaps bringing Gemini into the conversation! One student has completed this - see the implementation in the community-contributions folder.\n",
    "\n",
    "The most reliable way to do this involves thinking a bit differently about your prompts: just 1 system prompt and 1 user prompt each time, and in the user prompt list the full conversation so far.\n",
    "\n",
    "Something like:\n",
    "\n",
    "```python\n",
    "system_prompt = \"\"\"\n",
    "You are Alex, a chatbot who is very argumentative; you disagree with anything in the conversation and you challenge everything, in a snarky way.\n",
    "You are in a conversation with Blake and Charlie.\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = f\"\"\"\n",
    "You are Alex, in conversation with Blake and Charlie.\n",
    "The conversation so far is as follows:\n",
    "{conversation}\n",
    "Now with this, respond with what you would like to say next, as Alex.\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "Try doing this yourself before you look at the solutions. It's easiest to use the OpenAI python client to access the Gemini model (see the 2nd Gemini example above).\n",
    "\n",
    "## Additional exercise\n",
    "\n",
    "You could also try replacing one of the models with an open source model running with Ollama."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446c81e3-b67e-4cd9-8113-bc3092b93063",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/business.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#181;\">Business relevance</h2>\n",
    "            <span style=\"color:#181;\">This structure of a conversation, as a list of messages, is fundamental to the way we build conversational AI assistants and how they are able to keep the context during a conversation. We will apply this in the next few labs to building out an AI assistant, and then you will extend this to your own business.</span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23224f6-7008-44ed-a57f-718975f4e291",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
